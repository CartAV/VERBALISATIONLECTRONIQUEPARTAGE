#!/bin/sh

#recette patch en attendant l'injecteur ES5

dataset=2010_2015_pve_sr # nom de l'index ES 
type=event #type du document
server=http://fa-srv-5:9200 # ip du serveur d'injection 
split=10000 # taille des bulks (peut ralentir ou accélérer la recherche ; diviser par deux pour la véritable taille des bulks e.g 10000 lignes = 5000 documents insérés
refresh_interval="10s" #pour accélérer l'indexation (latence de l'actualisation de la recherche après l'indexation - valeur de base "1s"
number_of_replicas=5  #plus de replicas accélère la recherche mais ralentit l'indexation de base 2; 5 max (nombre de noeuds du cluster actuel)
columns=".*" #selection des colonnes à retenir (match type regexp : col_1|col_2 ...)

# initalisation de l'index

curl -s -XDELETE $server/$dataset
curl -s -XPUT $server/$dataset
#refresh_interval : pour accélérer l'indexation ; number_of_replicas : ralentit l'indexation, accélère la recherche après
curl -s -XPUT $server/$dataset/_settings -d '{"index":{"refresh_interval": "'"$refresh_interval"'","number_of_replicas" :'$number_of_replicas'}}'

curl -s -XPUT $server/$dataset -H 'Content-Type: application/json' -d'
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "edge_ngram",
          "min_gram": 2,
          "max_gram": 10,
          "token_chars": [
            "letter"
          ]
        }
      }
    }
  }
}'

#########
#variante 1 process passant par le buffer mémoire 
#semble échouer au-delà de 5M de ligne du fait du process java encapsulant sh, jvm trop petite malgré un espace disponible
#cat | tr '\t' ';' |perl -e '$header=1;while(<>){s/\"/\\\"/g;s/\t//; chomp;if ($header) {@fields=split(/;/,$_);@selected=map {(/'"$columns"'/?1:0)} @fields;$header=0; } else {print "{\"index\": {\"_index\": \"'"$dataset"'\", \"_type\": \"'"$type"'\"}}\n";$i=0;print "{".join(", ",grep {$_} map{(@selected[$i++]==1?"\"@fields[$i-1]\": \"$_\"":undef)} split(/;/,$_))."}\n";}}'  | split -l $split  - --filter "curl -s -XPOST $server/_bulk --data-binary @- 2> /dev/null"
#exit 0

########
#variante 2 process passant par un buffer disque

mkdir -p bulk
# prépare le bulk à partir de STDIN (DSS renvoie un CSV avec "\t" comme séparateur"
cat | tr '\t' ';' | perl -e '$header=1;while(<>){s/\"/\\\"/g;s/\t//; chomp;if ($header) {@fields=split(/;/,$_);@selected=map {(/'"$columns"'/?1:0)} @fields;$header=0; } else {print "{\"index\": {\"_index\": \"'"$dataset"'\", \"_type\": \"'"$type"'\"}}\n";$i=0;print "{".join(", ",grep {$_} map{(@selected[$i++]==1?"\"@fields[$i-1]\": \"$_\"":undef)} split(/;/,$_))."}\n";}}' | split -l $split  -a 5 -d - bulk/$dataset.ES.json. &
PID=$!
#test parsing
#head -2 bulk/$dataset.ES.json.0000 | tail -1 | jq '.'
#exit
sleep 5

# en parallèle de la préparation des bulks, commence à insérer dans ES les bulks déjà préparés
cd bulk
while (ps -p $PID > /dev/null)
do echo "loop"
for file in $dataset.ES.json.*
do test=`wc -l $file| awk '{printf $1}'`
if [[ $test -eq $split ]]
then
echo "$file"
#debug :
(cat $file | curl -s -XPOST $server/_bulk --data-binary @- 2| jq -c '.items[]' | gzip > "$file.log.gz")
#cat $file | curl -s -XPOST $server/_bulk --data-binary @- 2> /dev/null
rm "$file"
rm "$file.log.gz"
fi
done
sleep 5
done


echo "last loop"
for file in $dataset.ES.json.????? 
do echo "$file"
#debug :
(cat $file | curl -s -XPOST $server/_bulk --data-binary @- 2| jq -c '.items[]' | gzip > "$file.log.gz")
#cat $file | curl -s -XPOST $server/_bulk --data-binary @- 2> /dev/null
rm "$file"
rm "$file.log.gz"

done

exit 1
